#from crawler import crawler
#from pagerank import page_rank
import redis
import pymongo
from pymongo import MongoClient

connection_params={
    'user': 'Deep297',
    'password': 'seek-search3',
    'host': 'ds111244.mlab.com',
    'port': '11244',
    'namespace': 'seek_search-engine',
}

connection = MongoClient('mongodb://Deep297:seek-search3@ds111244.mlab.com:'
                         '11233/seek_search-engine'.format(**connection_params))
db = connection['seek_search-engine']
db.authenticate('Deep297', 'seek-search3')
print (db.collection_names())

# Get crawler object and crawl on urls found in urls.txt
crawler = crawler(None, 'urls.txt')
crawler.crawl()
# Get the data structures generated by the crawler
lexicon = crawler.get_lexicon()
inverted_index = crawler.get_inverted_index()
resolved_inverted_index = crawler.get_resolved_inverted_index()
document_index = crawler.get_document_index()
# Run pagerank on the links generated by the crawler
pagerank = page_rank(crawler._links)
# Store data on persistent storage i.e. Redis
rdb = redis.Redis()
rdb.flushdb()

for word in lexicon:
    rdb.set('lexicon:' + str(word), lexicon[word])
for word_id in inverted_index:
    rdb.set('inverted_index:' + str(word_id), str(list(inverted_index[word_id])).strip('[]'))
for word in resolved_inverted_index:
    rdb.set('resolved_inverted_index:' + str(word), str(list(resolved_inverted_index[word])).strip('[]'))
for doc_id in document_index:
    doc = document_index[doc_id]
    rdb.set('url:' + str(doc_id), doc[0])
    rdb.set('title:' + str(doc_id), doc[1])
    try:
    	rdb.set('description:' + str(doc_id), doc[2])
    except:
    	rdb.set('words:' + str(doc_id), doc[3])
for doc_id in pagerank:
    rdb.set('pagerank:' + str(doc_id), pagerank[doc_id])

rdb.save()
